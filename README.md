This is the project repo for the final project of the Udacity Self-Driving Car Nanodegree: Programming a Real Self-Driving Car. For more information about the project, see the project introduction [here](https://classroom.udacity.com/nanodegrees/nd013/parts/6047fe34-d93c-4f50-8336-b70ef10cb4b2/modules/e1a23b06-329a-4684-a717-ad476f0d8dff/lessons/462c933d-9f24-42d3-8bdc-a08a5fc866e4/concepts/5ab4b122-83e6-436d-850f-9f4d26627fd9).

[//]: # (Image References)

[arch1]: ./writeup_images/final-project-ros-graph-v2.png "ROS architecture"
[stop1]: ./writeup_images/stop_at_red_light.png "Stop at red light"

[infer1]: ./writeup_images/red_center.png "Red light large."
[infer2]: ./writeup_images/red_distance.png "Red far away"
[infer3]: ./writeup_images/incorrect_yellow.png "Red light large."
[infer4]: ./writeup_images/incorrect_red_redgreen.png "Red light large."

### Setup

This project is currently running on a much older 2011 macbook pro.  The
simulator runs native on the macbook and the ROS packages and inference
run on a virtualbox.  The training and inference testing was done on an
AWS GPU instance.

I would not recommend this setup for future use.  The older macbook had
problems with the load generated by the simulator.  This appears to be a
common problem based on the discussion boards.

The virtual box usage was reasonable and workable.

## It Stops!!!  /   Requirements

![alt text][stop1]

It also meets the other requirements of:
- Smoothly following waypoints
- Respents the top speed set for the waypoints
- It obeys the use of dbw_enabled
- It publishes throttle steering and brake commands at 50hz
- It launches from the standard launch files.

As this project stands now it is running off the ground truth data for the traffic lights.  I have extreme trouble with the integration of the tl
detection based on the model that I chose to use(the tensorflow api).  Between
the load from the simulator and the inference, I have not been able to debug
a number of problems with the inference integration.  I will likely need a
native linux machine (as suggested) to make further progress.

## Software Architecture

ROS and external detection

## ROS components

The goal of this project is to implement key elements of the ROS architecture for both the simulated vehicle and the Udacity Carla vehicle.

The subset of the ROS arhictecture that we will be working with is diagrammed below:

![alt text][arch1]

### Perception

This section of the write up just covers the ROS implementation portions.  The work on traffic light detection with tensorflow API is covered below.

#### Traffic light detection

This ROS module, tl_detector.py, processes incoming /image_color and /current_pose messages to determine if a traffic light is ahead of the vehicle.
It publishes a /traffic_waypoint message to indicate the traffic light location.

The module has a few features to note:
- state machine to filter positive indications of a red light.  The light must be present for STATE_COUNT_THRESHOLD(3) updates before it is published.
- It has the ability to process ground truth available from the simulator to indicate traffic lights.
- It can also call out to the frozen_inference.py script to use a saved NN to provide the inference.

### Planning

The waypoint_updater.py code subscribes to /current_pose, /base_waypoints and /traffic_waypoint.  It's purpose is to publish a set of waypoints (next 200) that include speed goals for each waypoint that set desired speed for the situations of normal running and approach light or objects.

/base_waypoints handling is simpley storing the one-time published data for future use.

On every /current_pose update the code generates a set of /final_waypoints based on the current state from the incoming /traffic_waypoint data.

When a red light is seen ahead this code modifies the speed goals for the configurable number of waypoints(60 now) leading up to the stop light.  The actual waypoint published is the desired stop line ahead of the light.

One major performance issue was in determining the closest waypoint to the current position.  My machine was not capable of searching all waypoints.  I settled on a solution of searching nearby waypoints when I had a last known waypoint to start with.

### Control

For the waypoint follower, I used the default implementation that uses the pure_pursuit code.

My dbw_node.py code simply publishes the messages from the data supplied by the twist_controller.py module.

The twist_controller.py modules generates control values for acceleration, braking and steering.  It uses a PID controller for acceleration and a yaw
controller for steering.  I originally had the same PID contoller control
braking, but I was never able to get the braking behavior I desired.  I ended up with a simple relationship to the difference in desired velocity to the current velocity.

The braking behavior is still not ideal, it shows a fairly strong response to
small errors and quickly hits its maximum braking.  It also does not have any
relationship to the physical properties of the car.  I had quite a bit of trouble determinely the correct braking input values. They do not appear to be documented in the code, the course materials nor in the discussion boards.

The PID controller for acceleration appears to have good performance.  It has only been manually tuned base on relative values from one of the prior projects.

The yaw_controller for steering is effective.  It has some over controlling behavior at very low speeds.  This may also be due to a very close waypoint.

## Traffic light detection

The decision to use the tensorflow API object detection models was somewhat
arbitrary.  The primary reason is that I wanted to have exposure to transfer
learning with the tensorflow contributions framework.

I primarily used the 'ssd_inception_v2_coco_2017_11_17' model.

### Dataset

My initial data set was thankfully provided by [Anthony Sarkis](https://medium.com/@anthony_sarkis).  The dataset he has made available was composed
of 280 images with 670 bounding boxes of identified lights.  I was only using 
the simulator based data.

### Model setup

The model setup I was using was based directly out of the tensorflow/models
[Object Detection repository](https://github.com/tensorflow/models/tree/master/research/object_detection).  The previous repository includes a installation
instructions link.  NOTE: it is very important to follow the location placement in the instructions.

#### Data preperation
The key setup components are reasonably well documented on the [Preparing Inputs link](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/preparing_inputs.md).  This provides the command and setup to transform the data to TF records that can actually be used.

My data conversion script data/data_conversion.ipynb is largely taken from 
the data_conversion_bosch.py script.  It is location specific and must be
placed into the tensoflow models/reasearch directory to run as is.

### Training

Training set for the tensorflow models API is largely documented at the
[Running locally link](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_locally.md).

With my setup the command is:

'''
python  ../models/research/object_detection/train.py --pipeline_config_path=./config/ssd_inception_v2_coco.udacity_sim.config --train_dir=./models/ssd_inception_v2_coco_2017_11_17/
'''

The next step is exporting the frozen inference graph:
'''
python ../models/research/object_detection/export_inference_graph.py --pipeline_config_path=config/ssd_inception_v2_coco.udacity_sim.config --trained_checkpoint_prefix=./models/train/model.ckpt-397 --output_directory=./models/frozen_models/frozen_ssd_inception_v2_coco.udacity_sim
'''

### Test inference

The performance on infering red lights is very good even at distances that make the pixel count composing the red light quite small.  The performance on green on yellow lights seems to suffer from either the lack of a balanced data set or simply insuficient data for those types.

![alt text][infer1]
![alt text][infer2]

The inference threshold for the following images was dropped in order to see
what is being detected.

![alt text][infer3]
![alt text][infer4]
